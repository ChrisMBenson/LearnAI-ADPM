{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Creating and Updating a Docker Image before Deployment as a Webservice\n\nThis notebook demonstrates how to make changes to an existing docker image, before deploying it as a webservice.  \n\nKnowing how to do this can be helpful, for example if you need to debug the execution script of a webservice you're developing, and debugging it involves several iterations of code changes.  In this case it is not an option to deploy your application as a webservice at every iteration, because the time it takes to deploy your service will significantly slow you down.  In some cases, it may be easier to simply run the execution script on the command line, but this not an option if your script accumulates data across individual calls.\n\nWe describe the following process:\n\n1. Configure your Azure Workspace.\n2. Create a Docker image, using the Azure ML SDK.\n3. Test your Application by running the Docker container locally.\n4. Update the execution script inside your running Docker container.\n5. Commit the changes in your Docker container to its image\n6. Update the image in the Azure Container Registry (ACR).\n7. Deploy your Docker image as an Azure Container Instance ([ACI](https://azure.microsoft.com/en-us/services/container-instances/)) Webservice.\n8. Test your Webservice.\n    \n> Several cells below are completely commented out. This is because they can only be run on Jupyter, but not on Azure Databricks.  If you do have access to Jupyter, we recommend to explore these cells, because they give you an insight into how to debug a docker container."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Prerequisites\n- You need to have an [Azure](https://azure.microsoft.com) subscription. You will also need to know a subscription_id and resource_group.  One way to discover those is to visit the [azure portal](https://portal.azure.com) and look use the same as those of the DSVM.\n\n**Note:** \n- This code was tested on a Data Science Virtual Machine ([DSVM](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/)), running Ubuntu Linux 16.04 (Xenial). **Do *not* try to run this notebook on *Databricks***, because you may run into trouble executing some of the shell and docker commands.\n- If you get an error message when trying to import `azureml` in the first cell below, you probably have to switch to using the correct kernel: `Python [conda env:amladpm]`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Configure your Azure Workspace\n\nWe start by selecting your workspace, and make sure we have access to it from here.  In order for this to work, make sure you followed the instructions for creating a workspace in your development environment:\n\n- [DSVM](../lab0.0_Setting_Up_Env/configure_environment_DSVM.ipynb)\n- [Aure Databricks](../lab0.0_Setting_Up_Env/configure_environment_ADB.ipynb)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# %matplotlib inline  \n\nimport os\nfrom azureml.core import Workspace\nimport pandas as pd\nimport urllib\n\nconfig_path = '..'\n\n# # run this if you are using Jupyter (instead of azure datarbicks)\n# config_path = os.path.expanduser('~')\n\nws = Workspace.from_config(path=os.path.join(config_path, 'aml_config', 'config.json'))\n\nws.get_details()",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Found the config file in: /home/nbuser/library/aml_config/config.json\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "{'id': '/subscriptions/5be49961-ea44-42ec-8021-b728be90d58c/resourceGroups/wopauli_AD/providers/Microsoft.MachineLearningServices/workspaces/myADworkspace',\n 'name': 'myADworkspace',\n 'location': 'westus2',\n 'type': 'Microsoft.MachineLearningServices/workspaces',\n 'workspaceid': '49d41f78-827e-4beb-b761-a4dedef895db',\n 'description': '',\n 'friendlyName': 'myADworkspace',\n 'creationTime': '2018-12-17T20:30:26.4447476+00:00',\n 'containerRegistry': '/subscriptions/5be49961-ea44-42ec-8021-b728be90d58c/resourcegroups/wopauli_ad/providers/microsoft.containerregistry/registries/myadworkacrgfwbjfiz',\n 'keyVault': '/subscriptions/5be49961-ea44-42ec-8021-b728be90d58c/resourcegroups/wopauli_ad/providers/microsoft.keyvault/vaults/myadworkkeyvaultcjoanhrb',\n 'applicationInsights': '/subscriptions/5be49961-ea44-42ec-8021-b728be90d58c/resourcegroups/wopauli_ad/providers/microsoft.insights/components/myadworkinsightsurtqgdes',\n 'identityPrincipalId': '4c7164d5-bd3e-45a7-ae3e-bf00743c882b',\n 'identityTenantId': '72f988bf-86f1-41af-91ab-2d7cd011db47',\n 'identityType': 'SystemAssigned',\n 'storageAccount': '/subscriptions/5be49961-ea44-42ec-8021-b728be90d58c/resourcegroups/wopauli_ad/providers/microsoft.storage/storageaccounts/myadworkstoragemthzzpzu'}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's make sure that you have the correction version of the Azure ML SDK installed on your workstation or VM.  If you don't have the write version, please follow these [Installation Instructions](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python#install-the-sdk)."
    },
    {
      "metadata": {
        "tags": [
          "check version"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml\n\n# display the core SDK version number\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Azure ML SDK Version:  1.0.2\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create a Docker image using the Azure ML SDK"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create a template execution script for your application\n\nWe are going to start with just a barebones execution script for your webservice.  This script will calculate the running average of numbers thrown at it.\n\nWe recommend that you execute the cells for your `score.py` scripts twice.\n\n1. With a `#` sign at the beginning of the first line. This way you can detect typos and syntax errors during execution.\n2. If the script runs OK, you can remove the `#` sign, to write the script to a file instead of executing it."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# #%%writefile score.py\n\n# import json # we use json in order to interact with the anomaly detection service via a RESTful API\n\n# # The init function is only run once, when the webservice (or Docker container) is started\n# def init():\n#     global running_avg, curr_n\n    \n#     running_avg = 0.0\n#     curr_n = 0\n    \n#     pass\n\n# # the run function is run everytime we interact with the service\n# def run(raw_data):\n#     \"\"\"\n#     Calculates rolling average according to Welford's online algorithm.\n#     https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online\n\n#     :param raw_data: raw_data should be a json query containing a dictionary with the key 'value'\n#     :return: runnin_avg (float, json response)\n#     \"\"\"\n#     global running_avg, curr_n\n    \n#     value = json.loads(raw_data)['value']\n#     n_arg = 5 # we calculate the average over the last \"n\" measures\n    \n#     curr_n += 1\n#     n = min(curr_n, n_arg) # in case we don't have \"n\" measures yet\n    \n#     running_avg += (value - running_avg) / n\n    \n#     return json.dumps(running_avg)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create environment file for your Conda environment\n\nNext, create an environment file (environment.yml) that specifies all the python dependencies of your script. This file is used to ensure that all of those dependencies are installed in the Docker image.  Let's assume your Webservice will require ``azureml-sdk``, ``scikit-learn``, and ``pynacl``."
    },
    {
      "metadata": {
        "tags": [
          "set conda dependencies"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_conda_package(\"scikit-learn\")\nmyenv.add_pip_package(\"pynacl==1.2.1\")\nmyenv.add_pip_package(\"pyculiarity\")\nmyenv.add_pip_package(\"pandas\")\nmyenv.add_pip_package(\"numpy\")\n\nwith open(\"environment.yml\",\"w\") as f:\n    f.write(myenv.serialize_to_string())",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Review the content of the `environment.yml` file."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "with open(\"environment.yml\",\"r\") as f:\n    print(f.read())",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "# Conda environment specification. The dependencies defined in this file will\n# be automatically provisioned for runs with userManagedDependencies=False.\n\n# Details about the Conda environment file format:\n# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\n\nname: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n    # Required packages for AzureML execution, history, and data preparation.\n  - azureml-defaults\n  - pynacl==1.2.1\n  - pyculiarity\n  - pandas\n  - numpy\n- scikit-learn\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create the initial Docker image\n\nThe next step is to create a docker image.  \n\nWe start by creating downloading a scoring script that we prepared for this course.  You can skip the details of this script, if you are in a hurry.  Briefly, it contains our solution to online anomaly detection from the previous lab."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# uncomment the below lines to see the solution\nfilename = 'AD_score.py'\nurllib.request.urlretrieve(\n    os.path.join('https://raw.githubusercontent.com/Azure/LearnAI-ADPM/master/solutions/', filename),\n    filename='score.py')\n\nwith open('score.py') as f:\n    print(f.read())",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "import pandas as pd\nfrom pyculiarity import detect_ts\nimport os\n# from pytictoc import TicToc\n# from pyculiarity.date_utils import date_format\n# import matplotlib.pyplot as plt\n# import seaborn\n# import datetime\nimport numpy as np\nimport json\nimport glob\n\ndef create_data_dict(data, sensors):\n    \"\"\"\n\n    :param data:\n    :return:\n    \"\"\"\n    data_dict = {}\n    for column in data.columns:\n        data_dict[column] = [data[column].values[0]]\n        if column in sensors:\n            data_dict[column + '_avg'] = [0.0]\n            data_dict[column + '_an'] = [False]\n\n    return data_dict\n\n\ndef init_df(data):\n    \"\"\"\n    Init DataFrame from one row of data\n    :param data:\n    :return:\n    \"\"\"\n\n    # data_dict = create_data_dict(data)\n\n    df = pd.DataFrame() #data=data_dict, index=data_dict['timestamp'])\n\n    return df\n\n\ndef append_data(df, data, sensors):\n    \"\"\"\n    We either add the data and the results (res_dict) of the anomaly detection to the existing data frame,\n    or create a new one if the data frame is empty\n    \"\"\"\n    data_dict = create_data_dict(data, sensors)\n\n    df = df.append(pd.DataFrame(data=data_dict, index=data_dict['timestamp']))\n\n    return df\n\n\ndef init():\n    global storage_location\n\n    storage_location = \"/tmp/output\"\n\n    if not os.path.exists(storage_location):\n        os.makedirs(storage_location)\n\n    # next, we delete previous output files\n    files = glob.glob(os.path.join(storage_location,'*'))\n    \n    for f in files:\n        os.remove(f)\n        \n        \ndef generate_stream(telemetry, n=None):\n    \"\"\"\n        n is the number of sensor readings we are simulating\n        \"\"\"\n\n    if not n:\n        n = telemetry.shape[0]\n\n    machine_ids = [1] # telemetry['machineID'].unique()\n    timestamps = telemetry['timestamp'].unique()\n\n    # sort test_data by timestamp\n    # on every iteration, shuffle machine IDs\n    # then loop over machine IDs\n\n    #t = TicToc()\n    for timestamp in timestamps:\n        #t.tic()\n        np.random.shuffle(machine_ids)\n        for machine_id in machine_ids:\n            data = telemetry.loc[(telemetry['timestamp'] == timestamp) & (telemetry['machineID'] == machine_id), :]\n            run(data)\n        #t.toc(\"Processing all machines took\")\n\n\ndef load_df(data):\n    machineID = data['machineID'].values[0]\n\n    filename = os.path.join(storage_location, \"data_w_anoms_ID_%03d.csv\" % machineID)\n    if os.path.exists(filename):\n        df = pd.read_csv(filename)\n        df['timestamp'] = pd.to_datetime(df['timestamp'], format=\"%Y-%m-%d %H:%M:%S\")\n    else:\n        df = init_df(data)\n\n    return df\n\n\ndef save_df(df):\n    \"\"\"\n\n    :param df:\n    :return:\n    \"\"\"\n    machine_id = df.ix[0, 'machineID']\n\n    filename = os.path.join(storage_location, \"data_w_anoms_ID_%03d.csv\" % machine_id)\n\n    df.to_csv(filename, index=False)\n\n\ndef running_avgs(df, sensors, window_size=24):\n    \"\"\"\n    Calculates rolling average according to Welford's online algorithm.\n    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online\n\n    This adds a column next to the column of interest, with the suffix '_<n>' on the column name\n\n    :param df: a dataframe with time series in columns\n    :param column: name of the column of interest\n    :param n: number of measurements to consider\n    :return: None\n    \"\"\"\n\n    curr_n = df.shape[0]\n    row_index = curr_n - 1\n    window_size = min(window_size, curr_n)\n\n    for sensor in sensors:\n        val_col_index = df.columns.get_loc(sensor)\n        avg_col_index = df.columns.get_loc(sensor + \"_avg\")\n\n        curr_value = df.ix[row_index, val_col_index]\n\n        if curr_n > 1:\n            prv_avg = df.ix[(row_index -1), avg_col_index]\n            df.ix[row_index, avg_col_index] = prv_avg + (curr_value - prv_avg) / window_size\n        else:\n            df.ix[row_index, avg_col_index] = curr_value\n\n\ndef run(data, window=14 * 24):\n    \"\"\"\n\n    :param data:\n    :param window:\n    :return:\n    \"\"\"\n\n    # set some parameters for the AD algorithm\n    alpha = 0.1\n    max_anoms = 0.05\n    only_last = None  # alternative, we can set this to 'hr' or 'day'\n\n    data = pd.read_json(json.loads(data)['data'])\n    # return json.dumps(data.columns)\n\n    sensors = ['volt','pressure','vibration', 'rotate']  # list(data.columns[2:])\n\n    # load dataframe\n    df = load_df(data)\n\n    # add current sensor readings to data frame, also adds fields for anomaly detection results\n    df = append_data(df, data, sensors)\n\n    # calculate running averages\n    running_avgs(df, sensors)\n\n    # note timestamp  so that we can update the correct row of the dataframe later\n    timestamp = data['timestamp'].values[0]\n\n    # we get a copy of the current (also last) row of the dataframe\n    current_row = df.loc[df['timestamp'] == timestamp, :]\n\n    \n    # determine how many sensor readings we already have\n    rows = df.shape[0]\n\n    # if the data frame doesn't have enough rows for our sliding window size, we just return (setting that we have no\n    #  anomalies)\n    if rows < window:\n        save_df(df)\n        json_data = current_row.to_json()\n        \n        return json.dumps(json_data)\n\n    \n    # determine the first row of the data frame that falls into the sliding window\n    start_row = rows - window\n\n    # a flag to indicate whether we detected an anomaly in any of the sensors after this reading\n    detected_an_anomaly = False\n\n    # we loop over the sensor columns\n    for column in sensors:\n        df_s = df.ix[start_row:rows, ('timestamp', column + \"_avg\")]\n\n        # pyculiarity expects two columns with particular names\n        df_s.columns = ['timestamp', 'value']\n\n        # we reset the timestamps, so that the current measurement is the last within the sliding time window\n        # df_s = reset_time(df_s)\n\n        # calculate the median value within each time sliding window\n        # values = df_s.groupby(df_s.index.date)['value'].median()\n\n        # create dataframe with median values etc.\n        # df_agg = pd.DataFrame(data={'timestamp': pd.to_datetime(values.index), 'value': values})\n\n        # find anomalies\n        results = detect_ts(df_s, max_anoms=max_anoms,\n                            alpha=alpha,\n                            direction='both',\n                            e_value=False,\n                            only_last=only_last)\n\n        # create a data frame where we mark for each day whether it was an anomaly\n        df_s = df_s.merge(results['anoms'], on='timestamp', how='left')\n\n        # mark the current sensor reading as anomaly Specifically, if we get an anomaly in the the sliding window\n        # leading up (including) the current sensor reading, we mark the current sensor reading as anomaly note,\n        # alternatively one could mark all the sensor readings that fall within the sliding window as anomalies.\n        # However, we prefer our approach, because without the current sensor reading the other sensor readings in\n        # this sliding window may not have been an anomaly\n        # current_row[column + '_an'] = not np.isnan(df_agg.tail(1)['anoms'].iloc[0])\n        if not np.isnan(df_s.tail(1)['anoms'].iloc[0]):\n            current_row.ix[0,column + '_an'] = True\n            detected_an_anomaly = True\n\n    # It's only necessary to update the current row in the data frame, if we detected an anomaly\n    if detected_an_anomaly:\n        df.loc[df['timestamp'] == timestamp, :] = current_row\n    save_df(df)\n    \n    json_data = current_row.to_json()\n    \n    return json.dumps(json_data)\n\n\ndef main():\n    print(\"Reading data ... \", end=\"\")\n    telemetry = pd.read_csv(os.path.join('data', 'telemetry.csv'))  # , parse_dates=['datetime'])\n    print(\"Done.\")\n\n    print(\"Parsing datetime...\", end=\"\")\n    telemetry['datetime'] = pd.to_datetime(telemetry['datetime'], format=\"%m/%d/%Y %I:%M:%S %p\")\n    telemetry.columns = ['timestamp', 'machineID', 'volt', 'rotate', 'pressure', 'vibration']\n    print(\"Done.\")\n\n    print(\"Processing ... \", end=\"\")\n    init()\n    generate_stream(telemetry)\n    print(\"Done.\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%time\n\nfrom azureml.core.image import ContainerImage\n\n# configure the image\nimage_config = ContainerImage.image_configuration(execution_script = \"score.py\", \n                                                  runtime = \"python\",\n                                                  conda_file = \"environment.yml\")\n\n# create the docker image. this should take less than 5 minutes\nimage = ContainerImage.create(name = \"my-docker-image\",\n                              image_config = image_config,\n                              models = [],\n                              workspace = ws)\n\n# we wait until the image has been created\nimage.wait_for_creation(show_output=True)\n\n# let's save the image location\nimageLocation = image.serialize()['imageLocation']",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Creating image\nRunning..............................\nSucceededImage creation operation finished for image my-docker-image:1, operation \"Succeeded\"\nCPU times: user 2.79 s, sys: 433 ms, total: 3.22 s\nWall time: 3min 34s\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Test your Application by running the Docker container locally\n\n### Download the created Docker image from the Azure Container Registry ([ACR](https://azure.microsoft.com/en-us/services/container-registry/))\n\nHere we use some [cell magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html) to exchange variables between python and bash."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# %%bash -s \"$imageLocation\" \n\n# # get the location of the docker image in ACR\n# imageLocation=$1\n\n# # extract the address of the repository within ACR\n# repository=$(echo $imageLocation | cut -f 1 -d \".\")\n\n# echo \"Attempting to login to repository $repository\"\n# az acr login --name $repository\n# echo\n\n# echo \"Trying to pull image $imageLocation\"\n# docker pull $imageLocation",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Start the docker container\n\nWe use standard Docker commands to start the container locally."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# %%bash -s \"$imageLocation\"\n\n# # extract image name and tag from imageLocation\n# image_name=$(echo $1 | cut -f 1 -d \":\")\n# tag=$(echo $1 | cut -f 2 -d \":\")\n# echo \"Image name: $image_name, tag: $tag\"\n\n# # extract image ID from list of downloaded docker images\n# image_id=$(docker images $image_name:$tag --format \"{{.ID}}\")\n# echo \"Image ID: $image_id\"\n\n# # we forward TCP port 5001 of the docker container to local port 8080 for testing\n# echo \"Starting docker container\"\n# docker run -d -p 8888:5001 $image_id\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Test the docker container\n\nWe test the docker container, by sending some data to it to see how it responds - just as we would with a Webservice.\n\n> If you get an error message below, you may just have to wait a couple of seconds."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# import json\n# import requests\n# import numpy as np\n# import matplotlib.pyplot as plt\n\n# values = np.random.normal(0,1,100)\n# values = np.cumsum(values)\n\n\n# running_avgs = []\n\n# for value in values:\n#     raw_data = {\"value\": value}\n\n#     r = requests.post('http://localhost:8888/score', json=raw_data)\n\n#     result = json.loads(r.json())\n#     running_avgs.append(result)\n\n# plt.close()\n# plt.plot(values)\n# plt.plot(running_avgs)\n# display()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Modifying the container\n\nLet's make a change to the the execution script: Copy the changed ``AD_score.py`` into the running docker container and commit the changes to the container image."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# %%bash -s $imageLocation\n\n# image_location=$1\n\n# # extract image name and tag from imageLocation\n# image_name=$(echo $image_location | cut -f 1 -d \":\")\n# tag=$(echo $image_location | cut -f 2 -d \":\")\n\n# echo \"Image name: $image_name, tag: $tag\"\n\n# # extract image id\n# image_id=$(docker images $image_name:$tag --format \"{{.ID}}\")\n\n# echo \"Image ID: $image_id\"\n\n# # extract container ID\n# container_id=$(docker ps | tail -n1 | cut -f 1 -d \" \")\n# echo \"Container ID: $container_id\"\n\n# # copy modified scoring script again\n# docker cp AD_score.py $container_id:/var/azureml-app/score.py\n# sleep 1\n\n# # stop the container\n# docker restart $container_id",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Test the container"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Load telemetry data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "base_path = 'https://sethmottstore.blob.core.windows.net'\ndata_dir = os.path.join(base_path, 'predmaint')\n\nprint(\"Reading data ... \", end=\"\")\ntelemetry = pd.read_csv(os.path.join(data_dir, 'telemetry.csv'))\nprint(\"Done.\")\n\nprint(\"Parsing datetime...\", end=\"\")\ntelemetry['datetime'] = pd.to_datetime(telemetry['datetime'], format=\"%m/%d/%Y %I:%M:%S %p\")\ntelemetry.columns = ['timestamp', 'machineID', 'volt', 'rotate', 'pressure', 'vibration']\nprint(\"Done.\")",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Reading data ... Done.\nParsing datetime...Done.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# import numpy as np\n# import json\n# import requests\n\n# def test_docker(telemetry, n=None):\n#     \"\"\"\n#         n is the number of sensor readings we are simulating\n#         \"\"\"\n\n#     if not n:\n#         n = telemetry.shape[0]\n\n#     machine_ids = [1] # telemetry['machineID'].unique()\n#     timestamps = telemetry['timestamp'].unique()\n\n#     out_df = pd.DataFrame()\n#     for timestamp in timestamps[:10]:\n#         np.random.shuffle(machine_ids)\n#         for machine_id in machine_ids:\n#             data = telemetry.loc[(telemetry['timestamp'] == timestamp) & (telemetry['machineID'] == machine_id), :]\n#             json_data = data.to_json()\n#             input_data = {\"data\": json_data}\n           \n#             r = requests.post('http://localhost:8888/score', json=input_data)\n\n#             result = pd.read_json(json.loads(r.json()))\n#             out_df = out_df.append(result, ignore_index=True)\n#     return out_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# print(\"Processing ... \")\n# out_df = test_docker(telemetry)\n# print(\"Done.\")\n# out_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Push the updated container to ACR\n\n**First**, test your Docker container again (run the json query above), to ensure that the changes are having the expected effect. \n\n**Then** you can push the image into ACR, so that it can be retrieved by the Azure ML SDK when you want to deploy your Webservice."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# %%bash -s \"$imageLocation\"\n\n# image_location=$1\n\n# # extract container ID\n# container_id=$(docker ps | tail -n1 | cut -f 1 -d \" \")\n# echo \"Container ID: $container_id\"\n\n# # commit changes made in the container to the local copy of the image\n# docker commit $container_id $image_location\n\n# docker push $image_location",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's try to deploy the container to ACI, just to make sure everything behaves as expected."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%time\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.image import ContainerImage\nfrom azureml.core.webservice import AciWebservice\n\n# create configuration for ACI\naciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=1, \n                                               tags={\"data\": \"some data\",  \"method\" : \"machine learning\"}, \n                                               description=\"Does machine learning on some data\")\n# pull the image\nimage = ContainerImage(ws, name='my-docker-image')\n\n# deploy webservice\nservice_name = 'my-web-service'\nservice = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                            image = image,\n                                            name = service_name,\n                                            workspace = ws)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Creating service\nRunning................\nSucceededACI service creation operation finished, operation \"Succeeded\"\nHealthy\nCPU times: user 1.65 s, sys: 291 ms, total: 1.95 s\nWall time: 1min 57s\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport json\nimport requests\n\ndef test_webservice(telemetry, n=None):\n    \"\"\"\n        n is the number of sensor readings we are simulating\n        \"\"\"\n\n    if not n:\n        n = telemetry.shape[0]\n\n    machine_ids = [1] # telemetry['machineID'].unique()\n    timestamps = telemetry['timestamp'].unique()\n\n    out_df = pd.DataFrame()\n    for timestamp in timestamps[:n]:\n        np.random.shuffle(machine_ids)\n        for machine_id in machine_ids:\n            data = telemetry.loc[(telemetry['timestamp'] == timestamp) & (telemetry['machineID'] == machine_id), :]\n            json_data = data.to_json()\n            input_data = bytes(json.dumps({\"data\": json_data}), encoding = 'utf8')\n    \n            result = pd.read_json(json.loads(service.run(input_data=input_data)))\n\n            out_df = out_df.append(result, ignore_index=True)\n    return out_df",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Processing ... \")\nout_df = test_webservice(telemetry, n=10)\nprint(\"Done.\")\nout_df",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Processing ... \n",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'service' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7d443406850e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing ... \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_webservice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtelemetry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mout_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-bb25b473ece2>\u001b[0m in \u001b[0;36mtest_webservice\u001b[0;34m(telemetry, n)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mout_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'service' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "service.serialize()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Clean up resources\n\nTo keep the resource group and workspace for other tutorials and exploration, you can delete only the ACI deployment using this API call:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "service.delete()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# The end"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "msauthor": "sgilley",
    "name": "AD_model_deployment",
    "notebookId": 1951564739234124
  },
  "nbformat": 4,
  "nbformat_minor": 1
}