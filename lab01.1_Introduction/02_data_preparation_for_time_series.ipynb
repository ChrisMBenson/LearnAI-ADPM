{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "In this lab, we will explore and visualize our telemetry data.  You will learn how calculate metrics on top of your raw time series to gain deeper insights into your data.  \n",
    "\n",
    "For example, sometimes the raw data looks just like noise, but if you run a [short-time Fourier transform](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) you are able to detect that there are systematic oscillations in your data.\n",
    "\n",
    "In this lab, you will:\n",
    "- Get to know your dataset better by visualizing it\n",
    "- Learn how to visualize time series data\n",
    "- Become familiar with a set of standard metrics that can be defined on time series data\n",
    "- Understand when to use which metric\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "You will need the following Python packages to run this lab:\n",
    "- rstl\n",
    "- scipy\n",
    "- seaborn\n",
    "\n",
    "We will guide you through the process of installing these packages.\n",
    "\n",
    "## Load and visualize/explore your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "# let's set up your environment, and define some global variables\n",
    "\n",
    "import os\n",
    "from rstl import STL\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import seaborn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "base_path = '..'\n",
    "data_subdir = 'data'\n",
    "data_filename = 'telemetry.csv'\n",
    "data_path = os.path.join(base_path, data_subdir, data_filename)\n",
    "\n",
    "# adjust this based on your screen's resolution\n",
    "fig_panel = (18, 16)\n",
    "wide_fig = (16, 4)\n",
    "dpi=80 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we load the telemetry data\n",
    "\n",
    "print(\"Reading data ... \", end=\"\")\n",
    "df = pd.read_csv(data_path)\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Parsing datetime...\", end=\"\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "print(\"Done.\")\n",
    "\n",
    "df = df.rename(str, columns={'datetime': 'timestamp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define some useful variables\n",
    "sensors = df.columns[2:].tolist() # a list containing the names of the sensors\n",
    "machines = df['machineID'].unique().tolist() # a list of our machine ids\n",
    "\n",
    "n_sensors = len(sensors)\n",
    "n_machines = len(machines)\n",
    "print(\"We have %d sensors: %s for each of %d machines.\" % (n_sensors, sensors, n_machines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's pick a random machine\n",
    "random_machine = 67\n",
    "\n",
    "df_s = df.loc[df['machineID'] == random_machine, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get some info about the time domain\n",
    "df_s['timestamp'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: At which frequency do we receive sensor data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table of descriptive statistics for our data set\n",
    "df_s.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some time series specific exploration of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 24*14 # we look at the first 14 days of sensor data\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=fig_panel, dpi=dpi) # create 2x2 panel of figures\n",
    "for s, sensor in enumerate(sensors):\n",
    "    c = s%2 # column of figure panel\n",
    "    r = int(s/2) # row of figure panel\n",
    "    ax[r,c].plot(df_s['timestamp'][:n_samples], df_s[sensor][:n_samples])\n",
    "    ax[r,c].set_title(sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create histogram plots to have an understanding of how these data are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins=200\n",
    "\n",
    "fig, ax = plt.subplots(2,2,figsize=fig_panel, dpi=dpi)\n",
    "for s, sensor in enumerate(sensors):\n",
    "    c = s%2\n",
    "    r = int(s/2)\n",
    "    # fit normal distribution to data\n",
    "    (mu, sigma) = norm.fit(df_s[sensor])\n",
    "    \n",
    "    # the histogram of the data\n",
    "    n, bins, patches = ax[r,c].hist(df_s[sensor], density=True, bins=n_bins) \n",
    "    \n",
    "    # add a 'best fit' line\n",
    "    y = norm.pdf(bins, mu, sigma)\n",
    "    l = ax[r,c].plot(bins, y, linewidth=2)\n",
    "    ax[r,c].set_title(sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful metrics for Timeseries data\n",
    "\n",
    "## Bollinger Bands\n",
    "\n",
    "[Bollinger Bands](https://en.wikipedia.org/wiki/Bollinger_Bands) are a type of statistical chart characterizing the prices and volatility over time of a financial instrument or commodity, using a formulaic method propounded by John Bollinger in the 1980s. Financial traders employ these charts as a methodical tool to inform trading decisions, control automated trading systems, or as a component of technical analysis. \n",
    "\n",
    "This can be done very quickly with pandas, because it has a built-in function `ewm` for convolving the data with a sliding window with exponential decay, which can be combined with standard statistical functions, such as `mean` or `std`.\n",
    "\n",
    "Of course, you can imagine that rolling means, standard deviations etc can be useful on their own, without using them for creating Bollinger Bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 12 # the size of the window over which to aggregate\n",
    "sample_size = 24 * 7 * 2 # let's only look at two weeks of data\n",
    "x = df_s['timestamp']\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=fig_panel, dpi=dpi)\n",
    "for s, sensor in enumerate(sensors):\n",
    "    c = s%2\n",
    "    r = int(s/2)\n",
    "    rstd = df_s[sensor].ewm(window_size).std()\n",
    "    rm = df_s[sensor].ewm(window_size).mean()\n",
    "    ax[r,c].plot(x[window_size:sample_size], df_s[sensor][window_size:sample_size], color='blue', alpha=.2)\n",
    "    ax[r,c].plot(x[window_size:sample_size], rm[window_size:sample_size] - 2 * rstd[window_size:sample_size], color='grey')\n",
    "    ax[r,c].plot(x[window_size:sample_size], rm[window_size:sample_size] + 2 * rstd[window_size:sample_size], color='grey')\n",
    "    ax[r,c].plot(x[window_size:sample_size], rm[window_size:sample_size], color='black')\n",
    "    ax[r,c].set_title(sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag features\n",
    "\n",
    "Lag features can be very useful in machine learning approaches dealing with time series.  For example, if you want to train a model to predict whether a machine is going to fail the next day, you can just shift your logs of failures forward by a day, so that failures (i.e. target labels) are aligned with the feature data you will use for predicting failures.\n",
    "\n",
    "Luckily, pandas has a built-in `shift` function for doing this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 24 * 2 # let's only look at first two days\n",
    "x = df_s['timestamp']\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=fig_panel, dpi=dpi)\n",
    "for s, sensor in enumerate(sensors):\n",
    "    c = s%2\n",
    "    r = int(s/2)\n",
    "    rstd = df_s[sensor].ewm(window_size).std()\n",
    "    rm = df_s[sensor].ewm(window_size).mean()\n",
    "    ax[r,c].plot(x[:sample_size], df_s[sensor][:sample_size], color='black', alpha=1, label='orig')\n",
    "    ax[r,c].plot(x[:sample_size], df_s[sensor][:sample_size].shift(-1), color='blue', alpha=1, label='-1h') # shift by x hour\n",
    "    ax[r,c].plot(x[:sample_size], df_s[sensor][:sample_size].shift(-2), color='blue', alpha=.5, label='-2h') # shift by x hour\n",
    "    ax[r,c].plot(x[:sample_size], df_s[sensor][:sample_size].shift(-3), color='blue', alpha=.2, label='-3h') # shift by x hour\n",
    "    ax[r,c].set_title(sensor)\n",
    "ax[r,c].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling entropy\n",
    "\n",
    "Depending on your use-case entropy can also be a useful metric, as it gives you an idea of how evenly your measures are distributed in a specific range. For more information, visit Wikipedia:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Entropy_(information_theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "sample_size = 24*7*4 # use the first x hours of data\n",
    "\n",
    "sensor = 'volt'\n",
    "sensor_data = df_s[sensor]\n",
    "rolling_entropy = sensor_data.rolling(12).apply(entropy)\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=wide_fig)\n",
    "ax[0].plot(x[:sample_size], sensor_data[:sample_size])\n",
    "ax[1].plot(x[:sample_size], rolling_entropy[:sample_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short-time Fourier Transform (STFT)\n",
    "\n",
    "STFTs can be useful if you want to see whether there are oscillations in your data.  They can be used as a way of quantifying the change of a non-stationary signal’s frequency and phase content over time. It can be very useful to detect oscillations in a specific frequency range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "# Number of samplepoints\n",
    "N = df_s.shape[0]\n",
    "\n",
    "# sample spacing\n",
    "T = 1.0 / N\n",
    "x = df_s['timestamp']\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=fig_panel, dpi=dpi)\n",
    "for s, sensor in enumerate(sensors):\n",
    "    c = s%2 # column in figure panel\n",
    "    r = int(s/2) # row in figure panel\n",
    "    f, t, Zxx = signal.stft(df_s[sensor], fs=3600, nperseg=48)\n",
    "    ax[r,c].pcolormesh(t/t[1], f, np.abs(Zxx))\n",
    "    ax[r,c].set_title(sensor)\n",
    "    ax[r,c].set_ylabel('Frequency [Hz]')\n",
    "    ax[r,c].set_xlabel('Time (days)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your usecase, you may only be interested in the results within certain frequency ranges.  For example, you may only be interested in changes at the lowest frequency range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "freq_comp = 0\n",
    "freq_ts = np.abs(Zxx[freq_comp,:]) # \n",
    "\n",
    "# compare this to the bottom/right panel of the previous figure\n",
    "plt.plot(t/t[1], freq_ts)\n",
    "plt.xlabel('Time (days)')\n",
    "plt.ylabel('Power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other useful metrics\n",
    "\n",
    "There are various other useful metrics for timeseries data.  You may keep them in the back of your mind when you are dealing with another scenario.\n",
    "\n",
    "- Rolling median, min, max, mode etc. statistics\n",
    "- Rolling entropy, or rolling majority, for categorical features\n",
    "- Rolling text statistics for text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "The big question is when to use which metric for your use-case.  \n",
    "\n",
    "Here are a couple of sample scenarios. Can you recommend which one of the above metrics to use in each case?\n",
    "1. You are developing a fitness application for mobile phones that have an [accelerometer](https://en.wikipedia.org/wiki/Accelerometer). You want to be able to measure how much time a user spends sitting, walking, and running over the course of a day. Which metric would you use to identify the different activities?\n",
    "2. You want to get rich on the stock market, but you hate volatility.  Which metric would you use to measure volatility?\n",
    "3. You are in charge of a server farm.  You are looking for a way to detect denial of service attacks on your servers.  You don't want to constantly look at individual amounts of traffic at all of the servers at the same time.  However, you know that all of the servers typically get a constant amount of traffic.  Which metric could you use to determine that things have shifted, such as when some servers seem to be getting a lot more traffic than the other servers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyculiarity_p3]",
   "language": "python",
   "name": "conda-env-pyculiarity_p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
